{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "   \n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError):\n",
    "        pass\n",
    "    \n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    \n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tags(text, striptag=True):\n",
    "    \n",
    "    pattern = '#\\S+'\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = strip_accents(text)\n",
    "    \n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    if striptag :\n",
    "        matches = [ match.replace('#','') for match in matches ]\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2posts(json_info, infilter=False):\n",
    "\n",
    "    posts_list = json_info['graphql']['hashtag']['edge_hashtag_to_media']['edges']\n",
    "\n",
    "    posts_dicts = []\n",
    "    \n",
    "    # a generic media post preffix (concat with media shortcode to view)\n",
    "    posturl_prefix = 'https://www.instagram.com/p/'\n",
    "\n",
    "    for post in posts_list:\n",
    "\n",
    "        node = post['node']\n",
    "\n",
    "        id_post = node['id']\n",
    "\n",
    "        id_owner = node['owner']['id']\n",
    "\n",
    "        shortcode = node['shortcode']\n",
    "\n",
    "        edges = node['edge_media_to_caption']['edges']\n",
    "        \n",
    "        text = edges[0]['node']['text'].replace('\\n','') if len(edges) else ''\n",
    "        \n",
    "        tags = text2tags(text)\n",
    "\n",
    "        post_url = posturl_prefix + shortcode + '/'\n",
    "\n",
    "        post_dict = {\n",
    "            'id_post': id_post,\n",
    "            'id_owner': id_owner,\n",
    "            'shortcode': shortcode,\n",
    "            'text': text,\n",
    "            'post_url': post_url,\n",
    "            'tags': tags\n",
    "        }\n",
    "        \n",
    "        if infilter :\n",
    "            if len(tags) :\n",
    "                posts_dicts.append( post_dict )\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    else:\n",
    "        posts_dicts.append( post_dict )\n",
    "    \n",
    "    return posts_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowball(url, deep=1, end_cursor='', count=0, showurl=False, \n",
    "             sleep=0, forever=False, progress=False, pause=60 ):\n",
    "\n",
    "    # suffix to end cursor when requesting posts by tag\n",
    "    tagurl_endcursor = '&max_id='\n",
    "\n",
    "    request_url = url + tagurl_endcursor + end_cursor\n",
    "\n",
    "    if showurl :\n",
    "        print(request_url)\n",
    "    else:\n",
    "        if progress :\n",
    "            print( count, end=' ' )\n",
    "    \n",
    "    while True :\n",
    "        try :\n",
    "            json_info = requests.get( request_url ).json()\n",
    "            break\n",
    "        except:\n",
    "            if forever :\n",
    "                print('Fail, retrying in ' + str(pause) + ' seconds')\n",
    "                time.sleep(pause)\n",
    "            else:\n",
    "                print('Fail, ' + str(count) + ' requests done')\n",
    "                return []\n",
    "    \n",
    "    end_cursor = json_info['graphql']['hashtag']['edge_hashtag_to_media']['page_info']['end_cursor']\n",
    "\n",
    "    posts = json2posts( json_info, True )\n",
    "\n",
    "    time.sleep(sleep)\n",
    "  \n",
    "    count = count + 1\n",
    "\n",
    "    if count < deep :\n",
    "        posts += snowball(\n",
    "            url=url, \n",
    "            deep=deep, \n",
    "            end_cursor=end_cursor, \n",
    "            count=count, \n",
    "            showurl=showurl, \n",
    "            sleep=sleep,\n",
    "            forever=forever,\n",
    "            progress=progress, \n",
    "            pause=pause)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if showurl :\n",
    "        pass\n",
    "    else:\n",
    "        if progress :\n",
    "            if count == deep :\n",
    "                print()\n",
    "\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tag(tag):\n",
    "    \n",
    "    \"\"\"\n",
    "    Checks if a tag is valid according to its contents and size\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_LEN = 25\n",
    "    MIN_LEN = 1\n",
    "\n",
    "    pattern = '^[a-zA-Z0-9]+$'\n",
    "    \n",
    "    if re.match(pattern, tag) and len(tag) < MAX_LEN and len(tag) > MIN_LEN :\n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instagram base url preffix\n",
    "tagurl_prefix = 'https://www.instagram.com/explore/tags/'\n",
    "\n",
    "# suffix to append to tag request url to retrieve data in JSON format\n",
    "tagurl_suffix = '/?__a=1'\n",
    "    \n",
    "# target initial tags\n",
    "tags = ['IPL', 'CSK', 'RCB']\n",
    "\n",
    "# urls to initial tags\n",
    "queries = [ tagurl_prefix + tag + tagurl_suffix for tag in tags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying IPLfor depth of 4 pages...\n",
      "0 1 2 3 \n",
      "Total Number of posts 276\n",
      "Finished querying for IPL\n",
      "\n",
      "Now waiting for 30 seconds before querying for the next tag.\n",
      "Querying CSKfor depth of 4 pages...\n",
      "0 1 2 3 \n",
      "Total Number of posts 284\n",
      "Finished querying for CSK\n",
      "\n",
      "Now waiting for 30 seconds before querying for the next tag.\n",
      "Querying RCBfor depth of 4 pages...\n",
      "0 1 2 3 \n",
      "Total Number of posts 270\n",
      "Finished querying for RCB\n",
      "\n",
      "Now waiting for 30 seconds before querying for the next tag.\n",
      "CPU times: user 424 ms, sys: 0 ns, total: 424 ms\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = {}\n",
    "\n",
    "depth = 4 # Number of pages to query per post.\n",
    "\n",
    "for tag, query in zip( tags, queries ) :\n",
    "    \n",
    "    print( 'Querying ' + tag + 'for depth of '+ str(depth) + ' pages...' )\n",
    "    \n",
    "    posts = snowball(query, deep=depth, forever=True, sleep=1, pause=60, progress=True)\n",
    "    \n",
    "    print(\"Total Number of posts\", len(posts))\n",
    "    \n",
    "    data[tag] = posts\n",
    "    \n",
    "    print('Finished querying for ' + tag )\n",
    "    \n",
    "    print('\\nNow waiting for 30 seconds before querying for the next tag.')\n",
    "    \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data to a JSON file\n",
    "f = open('posts_data.json', 'w')\n",
    "json.dump(data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open/load the saved file.\n",
    "file = open('posts_data.json')\n",
    "data_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a limitation in the number of posts\n",
    "POSTS_MAX = 100\n",
    "\n",
    "# this list contains just edges from initial target (keys) tags to related post tags\n",
    "edges_list_keys = []\n",
    "\n",
    "# this list contains all edges between pairs of tags from the same post\n",
    "edges_list_all = []\n",
    "\n",
    "# populating the lists of edges\n",
    "for person, posts in data_json.items() :\n",
    "    \n",
    "    # traversing each post for each key tag\n",
    "    for post in posts[:POSTS_MAX] :\n",
    "        \n",
    "        # list of tags in the post including trash tags\n",
    "        post_tags = post['tags']\n",
    "        \n",
    "        # list of tags in the post after filtering\n",
    "        post_tags = [tag for tag in post_tags if validate_tag(tag)]\n",
    "        \n",
    "        # list of tags without the key tag\n",
    "        post_tags_drop_person = [tag for tag in post_tags if not tag == person]\n",
    "        \n",
    "        # creating edges between key tag and all others\n",
    "        for tag in post_tags_drop_person :\n",
    "            \n",
    "            edge_keys = (person, tag)\n",
    "            \n",
    "            edges_list_keys.append( edge_keys )\n",
    "        \n",
    "        # creating the edges between all the tags\n",
    "        for tag in post_tags :\n",
    "            \n",
    "            # index of the current tag in the list\n",
    "            tag_index = post_tags.index(tag)\n",
    "            \n",
    "            # this slice is needed in order to connect all edges one and only on time\n",
    "            post_tags_slice = post_tags[tag_index+1:]\n",
    "            \n",
    "            for stag in post_tags_slice :\n",
    "                \n",
    "                edge_all_pre = (tag, stag)\n",
    "                \n",
    "                # creating the edge element in alphabetical order\n",
    "                edge_all = ( min(edge_all_pre) , max(edge_all_pre) )\n",
    "                \n",
    "                edges_list_all.append( edge_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of edges:\n",
      "5249\n",
      "60946\n"
     ]
    }
   ],
   "source": [
    "print('Numbers of edges:')\n",
    "\n",
    "print(len(edges_list_keys))\n",
    "\n",
    "print(len(edges_list_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>1</td>\n",
       "      <td>england</td>\n",
       "      <td>indiancricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17129</th>\n",
       "      <td>1</td>\n",
       "      <td>love</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15669</th>\n",
       "      <td>2</td>\n",
       "      <td>justiceforsushant</td>\n",
       "      <td>viratmsdhoni7781mivscsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>1</td>\n",
       "      <td>hardikpandya</td>\n",
       "      <td>viratkohlifanpage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6765</th>\n",
       "      <td>3</td>\n",
       "      <td>csk</td>\n",
       "      <td>uae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       weight             source                   target\n",
       "9902        1            england            indiancricket\n",
       "17129       1               love                    style\n",
       "15669       2  justiceforsushant  viratmsdhoni7781mivscsk\n",
       "12284       1       hardikpandya        viratkohlifanpage\n",
       "6765        3                csk                      uae"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df = pd.DataFrame(edges_list_all, columns=['source', 'target'])\n",
    "edges_df['tuple'] = pd.Series(zip(edges_df.source, edges_df.target))\n",
    "\n",
    "# grouping the dataframe by tuple of source and target.\n",
    "edges_grouped = edges_df.groupby('tuple').count()\n",
    "edges_grouped.drop(columns='target', inplace=True, errors='ignore')\n",
    "edges_grouped.columns=['weight']\n",
    "edges_grouped.reset_index(inplace=True)\n",
    "\n",
    "# Adding source and target to the data frame.\n",
    "edges_grouped['source'] = edges_grouped.tuple.str[0]\n",
    "edges_grouped['target'] = edges_grouped.tuple.str[1]\n",
    "edges_grouped = edges_grouped.drop(columns='tuple')\n",
    "edges_grouped.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edges_grouped, edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Nodes length - 1287\n",
      "['1000families100likes', 'csection', 'csk', 'funnyvideos', 'likeforfollow', 'manishgoplani', 'memesdaily', 'sdvtodosnahora', 'share', 'stayhome']\n"
     ]
    }
   ],
   "source": [
    "print(\"Grouped Nodes length -\", len(G.nodes))\n",
    "print(list(G.nodes)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Edges length -  21684\n",
      "[('1000families100likes', 'csection', {'weight': 1}), ('1000families100likes', 'csk', {'weight': 1}), ('1000families100likes', 'funnyvideos', {'weight': 1}), ('1000families100likes', 'likeforfollow', {'weight': 1}), ('1000families100likes', 'manishgoplani', {'weight': 1}), ('1000families100likes', 'memesdaily', {'weight': 1}), ('1000families100likes', 'sdvtodosnahora', {'weight': 1}), ('1000families100likes', 'share', {'weight': 1}), ('1000families100likes', 'stayhome', {'weight': 1}), ('1000families100likes', 'taehyung', {'weight': 1})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Grouped Edges length - \", len(G.edges))\n",
    "print(list(G.edges(data=True))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out post with weights < 5\n",
      "     weight        source       target\n",
      "129       7          2020      cricket\n",
      "269       6    abdeallien          csk\n",
      "292       6  abdevilliers          csk\n",
      "293       6  abdevilliers      cskvsmi\n",
      "294       6  abdevilliers  davidwarner\n"
     ]
    }
   ],
   "source": [
    "# limiting weights > 5\n",
    "THRESHOLD = 5\n",
    "print('Filtering out post with weights < 5')\n",
    "mask_insignificant = edges_grouped.weight.apply(lambda x : x <= THRESHOLD)\n",
    "edges_grouped_dropped = edges_grouped[~mask_insignificant]\n",
    "print(edges_grouped_dropped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.from_edgelist(edges_list_keys)\n",
    "edges_df_keys = pd.DataFrame(edges_list_keys, columns=['source', 'target'])\n",
    "edges_df_keys['tuple'] = pd.Series(zip(edges_df_keys.source, edges_df_keys.target))\n",
    "edges_grouped_keys = edges_df_keys.groupby('tuple').count()\n",
    "edges_grouped_keys.drop(columns='target', inplace=True, errors='ignore')\n",
    "edges_grouped_keys.columns=['weight']\n",
    "edges_grouped_keys.reset_index(inplace=True)\n",
    "edges_grouped_keys['source'] = edges_grouped_keys.tuple.str[0]\n",
    "edges_grouped_keys['target'] = edges_grouped_keys.tuple.str[1]\n",
    "edges_grouped_keys = edges_grouped_keys.drop(columns='tuple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of weights\n",
    "node_weights = {}\n",
    "\n",
    "# populating the dictionary\n",
    "for person, posts in data_json.items() :\n",
    "    \n",
    "    for post in posts[:POSTS_MAX] :\n",
    "        \n",
    "        post_tags = post['tags']\n",
    "        \n",
    "        post_tags = [tag for tag in post_tags if validate_tag(tag)]\n",
    "        \n",
    "        for tag in post_tags :\n",
    "            \n",
    "            if tag in node_weights : \n",
    "                node_weights[tag] = node_weights[tag] + 1\n",
    "            else :\n",
    "                node_weights[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, node_weights, 'weight')\n",
    "nx.write_graphml(G, \"all_edges_nodes.graphml\") #needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new graph with dropped data\n",
    "G_dropped = nx.from_pandas_edgelist(edges_grouped_dropped, edge_attr=True)\n",
    "nx.set_node_attributes(G_dropped, node_weights, 'weight')\n",
    "nx.write_graphml(G_dropped, \"with_important_weights.graphml\") #needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the edges counted keys graph.\n",
    "g = nx.from_pandas_edgelist(edges_grouped_keys, edge_attr=True)\n",
    "nx.set_node_attributes(g, node_weights, 'weight')\n",
    "nx.write_graphml(g, \"with_tags.graphml\") # needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataWorks (Python + Spark)",
   "language": "python",
   "name": "dataworks_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
